{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18516bc8-8655-41e4-aa22-36f2a8b98042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Natural Language Processing (NLP) libraries\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Scikit-learn modeling libraries\n",
    "from sklearn.dummy import DummyClassifier # For baseline model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # To convert text to numbers\n",
    "from sklearn.linear_model import LogisticRegression # The classifier model\n",
    "from sklearn.metrics import accuracy_score, classification_report # For evaluation\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score # For splitting and validating\n",
    "from sklearn.pipeline import Pipeline # To chain processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e1693-4d0a-4127-ba6e-73bc772f8584",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1939d9f6-ed65-4f21-bd50-dc32d03dc914",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the training data from a JSON Lines file (one JSON object per line)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# The tweet data is nested. json_normalize flattens the nested JSON into columns.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_data \u001b[38;5;241m=\u001b[39m json_normalize(train_data\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\pandas\\io\\json\\_json.py:905\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m--> 905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\pandas\\io\\json\\_json.py:917\u001b[0m, in \u001b[0;36mJsonReader._preprocess_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 917\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n\u001b[0;32m    919\u001b[0m     data \u001b[38;5;241m=\u001b[39m StringIO(data)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the training data from a JSON Lines file (one JSON object per line)\n",
    "train_data = pd.read_json('train.jsonl', lines=True)\n",
    "# The tweet data is nested. json_normalize flattens the nested JSON into columns.\n",
    "train_data = json_normalize(train_data.to_dict(orient='records'))\n",
    "\n",
    "# Load the Kaggle test data (which we will make predictions on)\n",
    "kaggle_data = pd.read_json('kaggle_test.jsonl', lines=True)\n",
    "# Also normalize the Kaggle data\n",
    "kaggle_data = json_normalize(kaggle_data.to_dict(orient='records'))\n",
    "\n",
    "\n",
    "# Separate features from the target variable for the training set\n",
    "X_train = train_data.drop('label', axis=1)\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_kaggle = kaggle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70067757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement des jeux de donnÃ©es au format csv pour Ã©viter le temps de chargement futur\n",
    "\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "X_kaggle.to_csv('X_kaggle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c21670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_18332\\829428760.py:3: DtypeWarning: Columns (31,182,183,184,185,186,187,188,189,190,191) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  X_train = pd.read_csv('X_train.csv')\n",
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_18332\\829428760.py:5: DtypeWarning: Columns (181,182,183,184,185,186,187,188,189,190) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  X_kaggle = pd.read_csv('X_kaggle.csv')\n"
     ]
    }
   ],
   "source": [
    "# Test de chargement des jeux de donnÃ©es depuis les fichiers csv\n",
    "\n",
    "# X_train = pd.read_csv('X_train.csv')\n",
    "# y_train = pd.read_csv('y_train.csv')\n",
    "# X_kaggle = pd.read_csv('X_kaggle.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d30cb-5be1-4c5a-841c-915dad171f59",
   "metadata": {},
   "source": [
    "# 2. Transforming into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34854954-0088-4bb7-810f-77a5f2d7a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the full text from a tweet object.\n",
    "# Tweets can be truncated, storing the full version in 'extended_tweet.full_text'.\n",
    "def extract_full_text(tweet):\n",
    "    # Start with the standard 'text' field\n",
    "    text = tweet['text']\n",
    "    # Check if the 'extended_tweet.full_text' field exists (is not NaN)\n",
    "    if not pd.isna(tweet['extended_tweet.full_text']):\n",
    "        # If it exists, it's the full text, so use it instead\n",
    "        text = tweet['extended_tweet.full_text']\n",
    "    return text\n",
    "\n",
    "# Apply this function to every row (axis=1) in the training data\n",
    "X_train['full_text'] = X_train.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "# Apply the same function to the Kaggle test data\n",
    "X_kaggle['full_text'] = X_kaggle.apply(lambda tweet: extract_full_text(tweet), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07aa7648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renforcer les capacitÃ©s de dÃ©pistages et les actions de prÃ©vention : l'@ARS_ARA_SANTE et le @Prefet26 mandatent une Ã©quipe de mÃ©diateurs de lutte anti #COVID19 Ã  Grignan le 23 mars ðŸ”½https://t.co/B4zx89j1Gn\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "print(X_train['full_text'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37caea3-94f1-4861-9209-ad6cbd40ea09",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b61fcf-1103-474b-b803-d3b18bcfa3db",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Arthur/nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Arthur\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Arthur/nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Arthur\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load a list of common French stop words (e.g., 'le', 'la', 'de')\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m french_stop_words \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBuilding model pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a scikit-learn Pipeline. This chains steps together.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Data will flow from 'tfidf' (text to numbers) to 'clf' (classifier).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Arthur\\miniconda3\\envs\\inf554\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Arthur/nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Arthur\\\\miniconda3\\\\envs\\\\inf554\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Arthur\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Load a list of common French stop words (e.g., 'le', 'la', 'de')\n",
    "french_stop_words = stopwords.words('french')\n",
    "\n",
    "print(\"\\nBuilding model pipeline...\")\n",
    "\n",
    "# Create a scikit-learn Pipeline. This chains steps together.\n",
    "# Data will flow from 'tfidf' (text to numbers) to 'clf' (classifier).\n",
    "model_pipeline = Pipeline([\n",
    "    # Step 1: TfidfVectorizer - converts text into a matrix of TF-IDF features\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words=french_stop_words, # Remove French stop words\n",
    "        max_df=0.7,       # Ignore words that appear in > 70% of tweets (too common)\n",
    "        min_df=3,         # Ignore words that appear in < 3 tweets (too rare)\n",
    "        max_features=1000, # Keep only the top 1000 features\n",
    "        ngram_range=(1, 2)  # Include 1-word (unigrams) and 2-word (bigrams) sequences\n",
    "    )),\n",
    "    # Step 2: Classifier - Logistic Regression\n",
    "    ('clf', LogisticRegression(\n",
    "        random_state=42,    # For reproducible results\n",
    "        solver='liblinear'  # Good solver for this type of problem\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\nRunning 5-Fold Cross-Validation on training data...\")\n",
    "\n",
    "# Use StratifiedKFold to ensure class proportions are maintained in each fold\n",
    "# This is important for datasets that might be imbalanced\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# cross_val_score will train and test the pipeline 5 times\n",
    "# using the K-fold splits of the *training data*\n",
    "scores = cross_val_score(\n",
    "    model_pipeline,          # The pipeline to evaluate\n",
    "    X_train['full_text'],  # Features from training set\n",
    "    y_train,               # Labels from training set\n",
    "    cv=kfold,              # The stratified 5-fold splitter\n",
    "    scoring='accuracy'     # The metric to evaluate\n",
    ")\n",
    "\n",
    "# Print the cross-validation results\n",
    "print(f\"K-Fold Accuracy Scores: {scores}\")\n",
    "print(f\"Mean K-Fold Accuracy: {np.mean(scores) * 100:.2f}%\")\n",
    "print(f\"Std Dev K-Fold Accuracy: {np.std(scores) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining final model on all training data...\")\n",
    "# Now that we've validated the model, train it on ALL available training data\n",
    "model_pipeline.fit(X_train['full_text'], y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "print(\"\\n--- Final Model Evaluation on Held-Out Test Set ---\")\n",
    "# Use the trained pipeline to make predictions on the unseen Kaggle data\n",
    "# The pipeline automatically applies the TF-IDF transform and then predicts\n",
    "y_pred_test = model_pipeline.predict(X_kaggle['full_text'])\n",
    "\n",
    "# Prepare the submission file\n",
    "# Combine the 'challenge_id' from the Kaggle data with our predictions\n",
    "output = pd.concat([X_kaggle['challenge_id'], pd.DataFrame(y_pred_test)], axis=1,ignore_index=True)\n",
    "# Rename columns to match the required submission format\n",
    "output.columns = ['ID', \"Prediction\"]\n",
    "# Save the submission file as a CSV\n",
    "output.to_csv('logistic_regression.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523009f2-bace-4fb1-9ff4-fbfcebef187b",
   "metadata": {},
   "source": [
    "# 4. Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b582adf7-82d8-4eef-a02b-283305da1c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dummy (Most Frequent)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Dummy (Most Frequent)...\")\n",
    "# Create a DummyClassifier that always predicts the most frequent class\n",
    "# This is a baseline to see if our Logistic Regression model is actually learning anything\n",
    "dummy_mf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# \"Train\" the dummy model (it just finds the most frequent class in y_train)\n",
    "dummy_mf.fit(X_train['full_text'], y_train)\n",
    "\n",
    "# Make predictions on the Kaggle data (it will predict the same class for all rows)\n",
    "y_pred_test = dummy_mf.predict(X_kaggle['full_text'])\n",
    "\n",
    "# Prepare and save the dummy submission file\n",
    "output = pd.concat([X_kaggle['challenge_id'], pd.DataFrame(y_pred_test)], axis=1,ignore_index=True)\n",
    "output.columns = ['ID', \"Prediction\"]\n",
    "output.to_csv('dummy.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
